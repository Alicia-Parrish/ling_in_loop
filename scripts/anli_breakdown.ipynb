{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_val_summary(modifier, iteration, eval_dir, ):\n",
    "    fname = os.path.join(eval_dir, f'r{iteration}', 'tables', f'configs.{modifier}.csv')\n",
    "    summary_table = pd.read_csv(fname, index_col = 0)\n",
    "    summary_table = summary_table[[str(n) for n in range(1, iteration+1)]]\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "\n",
    "def get_itereval_summary(sub_keys, iteration, eval_dir, combined, ):\n",
    "    rep = {\n",
    "        '/': '-',\n",
    "        ';': '--',\n",
    "    }\n",
    "    \n",
    "    fname_key = '.'.join(sub_keys.values())\n",
    "    for old_char, new_char in rep.items():\n",
    "        fname_key = fname_key.replace(old_char, new_char)\n",
    "    fname = os.path.join(eval_dir, f'r{iteration}', 'tables', combined, f'iterevals.{fname_key}.csv')\n",
    "    summary_table = pd.read_csv(fname, index_col = 0)\n",
    "    summary_table = summary_table[[str(n) for n in range(1, iteration+1)]]\n",
    "    \n",
    "    return summary_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_mnli_tables(mnli_summary, subsetting='genre'):\n",
    "    with open(mnli_summary, 'r') as f:\n",
    "        summary = pd.DataFrame([json.loads(line) for line in f])\n",
    "    \n",
    "    mnli_tables = {}\n",
    "    for comb in summary['comb'].unique():\n",
    "        comb_sum = summary.loc[summary['comb'] == comb, :]\n",
    "\n",
    "        for subset in summary[subsetting].unique():\n",
    "            subset_sum = comb_sum.loc[comb_sum[subsetting] == subset, :]\n",
    "\n",
    "            plot_tab = []\n",
    "            for treat in subset_sum['treat'].unique():\n",
    "                treat_sum = subset_sum.loc[subset_sum['treat'] == treat, :]\n",
    "                s = treat_sum[['iter','acc']].set_index('iter').rename({'acc': treat}, axis=1).transpose()            \n",
    "                plot_tab.append(s)\n",
    "            \n",
    "            mnli_tables[(model, comb, subset)] = pd.concat(plot_tab)\n",
    "    \n",
    "    return summary, mnli_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_run_name(run_name, split_by='_'):\n",
    "    name_list = run_name.split(split_by)\n",
    "    if len(name_list) == 2:\n",
    "        input_type = 'full'\n",
    "        comb = 'combined'\n",
    "    elif len(name_list) == 3:\n",
    "        if name_list[-1] == 'hyp':\n",
    "            input_type = name_list[-1]\n",
    "            comb = 'combined'\n",
    "        else:\n",
    "            input_type = 'full'\n",
    "            comb = name_list[-1]\n",
    "    else:\n",
    "        input_type = name_list[-1]\n",
    "        comb = name_list[-2]\n",
    "\n",
    "    return (name_list[0], name_list[1], input_type, comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_sampled_results(sampled_base):\n",
    "    collected = pd.read_csv(os.path.join(sampled_base, 'collected.csv'))\n",
    "    itereval = pd.read_csv(os.path.join(sampled_base, 'itereval.csv'))\n",
    "    mnli = pd.read_csv(os.path.join(sampled_base, 'mnli.csv'))\n",
    "    anli = pd.read_csv(os.path.join(sampled_base, 'anli.csv'))\n",
    "    \n",
    "    \n",
    "    # fill in keys\n",
    "    collected['treat'] = collected['run'].apply(lambda x: split_run_name(x)[0])\n",
    "    collected['iter'] = collected['run'].apply(lambda x: int(split_run_name(x)[1]))\n",
    "    collected['mod'] = collected['run'].apply(lambda x: split_run_name(x)[2])\n",
    "    collected['combined'] = collected['run'].apply(lambda x: split_run_name(x)[3])\n",
    "    \n",
    "    mnli['breakdown'] = mnli['genre'].fillna('combined')\n",
    "    anli['breakdown'] = anli['tag'].fillna('combined')\n",
    "    \n",
    "    return collected, itereval, mnli, anli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_all_sampled(sampled_base, upto=5):\n",
    "    loaded_keys = {'collected': 0, 'itereval':1 ,'mnli': 2, 'anli': 3}\n",
    "    results = {key: [] for key in loaded_keys.keys()}\n",
    "    \n",
    "    for r in range(1, upto + 1):\n",
    "        loaded = load_sampled_results(os.path.join(sampled_base, f'r{r}'))\n",
    "        for result_key, loaded_key in loaded_keys.items():\n",
    "            results[result_key].append(loaded[loaded_key])\n",
    "    \n",
    "    return {\n",
    "        key: pd.concat(result_list, ignore_index=True)\n",
    "        for key, result_list in results.items()\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_ttest_pvals(dist_df, verbose=True):\n",
    "    pairs = [\n",
    "        ('baseline', 'LotS'),\n",
    "        ('baseline', 'LitL'),\n",
    "        ('LotS', 'LitL'),\n",
    "    ]\n",
    "    \n",
    "    ttest_dict = {}\n",
    "    for pair in pairs:\n",
    "        a = dist_df.loc[dist_df['treat'] == pair[0], 'acc']\n",
    "        b = dist_df.loc[dist_df['treat'] == pair[1], 'acc']\n",
    "        ttest_dict[pair] = ttest_ind(a, b)\n",
    "    \n",
    "    if verbose:\n",
    "        for pair, ttest_results in ttest_dict.items():\n",
    "            print('='*45)\n",
    "            print(f\"{pair}\\nt: {ttest_results[0]:.5f} | p: {ttest_results[1]/2:.5f}\")\n",
    "    \n",
    "    return ttest_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def err_line_plots(\n",
    "    plot_df,\n",
    "    ylim=[0,1],\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    tabletitle=None,\n",
    "    tableon=True,\n",
    "    x='iter',\n",
    "    y='acc',\n",
    "    hue='treat',\n",
    "    err_style='bars',\n",
    "    ci=95,\n",
    "    estimator=lambda x: np.median(x),\n",
    "    markers=True,\n",
    "    hue_order=['baseline', 'LotS', 'LitL'],\n",
    "    iteration=5,\n",
    "    bbox_to_anchor=(1.01, 1),\n",
    "    palette=None,\n",
    "    style_key='combined',\n",
    "    ax=None,\n",
    "):\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=plot_df, x=x, y=y,\n",
    "        hue=hue, err_style=err_style, ci=ci, markers=markers,\n",
    "        style = style_key, style_order = ['combined', 'separate'],\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_xticks(np.arange(1,6,1))\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.legend(bbox_to_anchor=bbox_to_anchor)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if not ax:\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='roberta-large-mnli'\n",
    "repo = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "eval_dir = os.path.join(repo, 'eval_summary', model)\n",
    "sample_type = 'cross_eval'\n",
    "iteration = 5\n",
    "\n",
    "mnli_summary = os.path.join(eval_dir, 'mnli_evals', 'eval_summaries.jsonl')\n",
    "anli_summary = os.path.join(eval_dir, 'anli_evals', 'eval_summaries.jsonl')\n",
    "\n",
    "plots_dir = os.path.join(eval_dir, 'sample', sample_type, f'final', 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_name = 'Performance'\n",
    "diff_name = 'Over Baseline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = load_all_sampled(\n",
    "    os.path.join(eval_dir, 'sample', sample_type), upto=iteration\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANLI Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_up = os.path.dirname(repo)\n",
    "anli_annot_fname = os.path.join(repo_up, 'anli_annot_v0.2_combined_A1A2')\n",
    "pred2annot_dicts = os.path.join(repo_up, 'data', 'ANLI_dicts.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "anli_annot = joblib.load(anli_annot_fname)\n",
    "\n",
    "with open(pred2annot_dicts, 'rb') as f:\n",
    "    pred2annot = pickle.load(f)['idxo2idxa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['c', 'e', 'n'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anli_annot['gold_label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic 0\n",
      "Basic 1327\n",
      "EventCoref 0\n",
      "EventCoref 66\n",
      "Imperfection 0\n",
      "Imperfection 453\n",
      "Numerical 0\n",
      "Numerical 1036\n",
      "Reasoning 0\n",
      "Reasoning 1977\n",
      "Reference 0\n",
      "Reference 868\n",
      "Tricky 0\n",
      "Tricky 893\n"
     ]
    }
   ],
   "source": [
    "breakdowns = [\n",
    "    'Basic',\n",
    "    'EventCoref',\n",
    "    'Imperfection',\n",
    "    'Numerical',\n",
    "    'Reasoning',\n",
    "    'Reference',\n",
    "    'Tricky',\n",
    "]\n",
    "\n",
    "for breakdown in breakdowns:\n",
    "    print(breakdown, anli_annot[breakdown].isna().sum())\n",
    "    print(breakdown, anli_annot[breakdown].ne('none').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anli_breakdowns(\n",
    "    preds,\n",
    "    anli_annotated,\n",
    "    pred2annot, \n",
    "    int2pred={0:'c', 1:'e', 2:'n'},\n",
    "    breakdowns = [\n",
    "        'combined',\n",
    "        'Basic',\n",
    "        'EventCoref',\n",
    "        'Imperfection',\n",
    "        'Numerical',\n",
    "        'Reasoning',\n",
    "        'Reference',\n",
    "        'Tricky',\n",
    "    ],\n",
    "    verbose=False,\n",
    "):\n",
    "    ans = {}\n",
    "    for breakdown in breakdowns:\n",
    "        ans[breakdown] = get_anli_acc(\n",
    "            preds,\n",
    "            anli_annotated,\n",
    "            pred2annot,\n",
    "            subset_col=breakdown,\n",
    "            verbose=verbose,\n",
    "            int2pred=int2pred,\n",
    "        )\n",
    "    return ans\n",
    "\n",
    "def get_anli_acc(\n",
    "    preds,\n",
    "    anli_annotated,\n",
    "    pred2annot, \n",
    "    int2pred={0:'contradiction', 1:'entailment', 2:'neutral'}, \n",
    "    subset_col=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    temp = anli_annotated\n",
    "    \n",
    "    skipped = 0\n",
    "    # get predictions\n",
    "    for idx, pred in enumerate(preds):\n",
    "        try:\n",
    "            temp.loc[pred2annot[idx], 'pred'] = int2pred[pred]\n",
    "        except KeyError as ke:\n",
    "            if verbose:\n",
    "                print(f'Warning: {ke}')\n",
    "            skipped += 1\n",
    "    if verbose:\n",
    "        print(skipped)\n",
    "    \n",
    "    assert temp['pred'].isna().sum() == 0\n",
    "    \n",
    "    temp['correct'] = temp['gold_label'] == temp['pred']\n",
    "    \n",
    "    if not subset_col is None and subset_col != 'combined':\n",
    "        temp = temp.loc[temp[subset_col].ne('none'), :]\n",
    "    \n",
    "    return temp['correct'].sum()/temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_partitions = np.linspace(0.1, 1, 10)\n",
    "treats = {\n",
    "    'baseline': '1_Baseline_protocol',\n",
    "    'LotS': '2_Ling_on_side_protocol',\n",
    "    'LitL': '3_Ling_in_loop_protocol',\n",
    "}\n",
    "rounds = range(1, 6)\n",
    "combineds = ['combined', 'separate']\n",
    "sampling = 'cross_eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 1.0 combined\n",
      "baseline 1.0 separate\n",
      "baseline 2.0 combined\n",
      "baseline 2.0 separate\n",
      "baseline 3.0 combined\n",
      "baseline 3.0 separate\n",
      "baseline 4.0 combined\n",
      "baseline 4.0 separate\n",
      "baseline 5.0 combined\n",
      "baseline 5.0 separate\n",
      "LotS 1.0 combined\n",
      "LotS 1.0 separate\n",
      "LotS 2.0 combined\n",
      "LotS 2.0 separate\n",
      "LotS 3.0 combined\n",
      "LotS 3.0 separate\n",
      "LotS 4.0 combined\n",
      "LotS 4.0 separate\n",
      "LotS 5.0 combined\n",
      "LotS 5.0 separate\n",
      "LitL 1.0 combined\n",
      "LitL 1.0 separate\n",
      "LitL 2.0 combined\n",
      "LitL 2.0 separate\n",
      "LitL 3.0 combined\n",
      "LitL 3.0 separate\n",
      "LitL 4.0 combined\n",
      "LitL 4.0 separate\n",
      "LitL 5.0 combined\n",
      "LitL 5.0 separate\n"
     ]
    }
   ],
   "source": [
    "save = True\n",
    "verbose = False\n",
    "\n",
    "pred_base = os.path.join(repo, 'predictions', model, 'anli_evals')\n",
    "\n",
    "anli_accs = []\n",
    "for treat, treat_dir in treats.items():\n",
    "    for r in rounds:\n",
    "        for combined in combineds:\n",
    "            print(treat, f'{r:.1f}', combined)\n",
    "            # breakdown for collected\n",
    "            ext_base = os.path.join(pred_base, treat_dir, f'r{r}', combined)\n",
    "            breakdowns = get_anli_breakdowns(\n",
    "                torch.load(os.path.join(ext_base, 'val_preds.p'))['mnli']['preds'],\n",
    "                anli_annot,\n",
    "                pred2annot,\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            for breakdown, acc in breakdowns.items():\n",
    "                anli_accs.append(\n",
    "                    {\n",
    "                        'treat': treat,\n",
    "                        'iter': int(r),\n",
    "                        'comb': combined,\n",
    "                        'breakdown': breakdown,\n",
    "                        'acc': acc,\n",
    "                        'sample_partition': None,\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            # breakdown for sampled collected\n",
    "            for sample_partition in sample_partitions:\n",
    "                extext_base = os.path.join(ext_base, sampling, f'{sample_partition:.1f}')\n",
    "                breakdowns = get_anli_breakdowns(\n",
    "                    torch.load(os.path.join(extext_base, 'val_preds.p'))['mnli']['preds'],\n",
    "                    anli_annot,\n",
    "                    pred2annot,\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "                for breakdown, acc in breakdowns.items():\n",
    "                    anli_accs.append(\n",
    "                        {\n",
    "                            'treat': treat,\n",
    "                            'iter': int(r),\n",
    "                            'comb': combined,\n",
    "                            'breakdown': breakdown,\n",
    "                            'acc': acc,\n",
    "                            'sample_partition': sample_partition,\n",
    "                        }\n",
    "                    )\n",
    "anli_accs_df = pd.DataFrame(anli_accs)\n",
    "if save:\n",
    "    os.makedirs(os.path.join(eval_dir, 'sample', sampling, 'final'), exist_ok=True)\n",
    "    anli_accs_df.to_csv(os.path.join(eval_dir, 'sample', sampling, 'final', 'anli_by_annotation.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
