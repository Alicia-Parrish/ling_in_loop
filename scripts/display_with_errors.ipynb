{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def my_plot(table, plot_args, title, xlabel='iteration', ylabel='accuracy', ylim=[0, 1]):\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    \n",
    "    lo = ax[0].plot(\n",
    "        table.columns.values,\n",
    "        table.transpose(),\n",
    "        **plot_args\n",
    "    )\n",
    "    ax[0].legend(iter(lo), table.index.values, loc='best')\n",
    "\n",
    "    ax[0].set_title(title)\n",
    "    ax[0].set_xlabel(xlabel)\n",
    "    ax[0].set_ylabel(ylabel)\n",
    "    ax[0].set_ylim(*ylim)\n",
    "    \n",
    "    cell_text = []\n",
    "    order = ['baseline', 'LotS', 'LitL']\n",
    "    for treat in order:\n",
    "        display_text = [f'{acc*100:.2f}%' for acc in table.loc[treat, :]]\n",
    "        cell_text.append(display_text)\n",
    "    \n",
    "    ax[1].table(cellText=cell_text, colLabels=table.columns, rowLabels=order, loc='center')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_val_summary(modifier, iteration, eval_dir, ):\n",
    "    fname = os.path.join(eval_dir, f'r{iteration}', 'tables', f'configs.{modifier}.csv')\n",
    "    summary_table = pd.read_csv(fname, index_col = 0)\n",
    "    summary_table = summary_table[[str(n) for n in range(1, iteration+1)]]\n",
    "    \n",
    "    return summary_table\n",
    "\n",
    "\n",
    "def get_itereval_summary(sub_keys, iteration, eval_dir, combined, ):\n",
    "    rep = {\n",
    "        '/': '-',\n",
    "        ';': '--',\n",
    "    }\n",
    "    \n",
    "    fname_key = '.'.join(sub_keys.values())\n",
    "    for old_char, new_char in rep.items():\n",
    "        fname_key = fname_key.replace(old_char, new_char)\n",
    "    fname = os.path.join(eval_dir, f'r{iteration}', 'tables', combined, f'iterevals.{fname_key}.csv')\n",
    "    summary_table = pd.read_csv(fname, index_col = 0)\n",
    "    summary_table = summary_table[[str(n) for n in range(1, iteration+1)]]\n",
    "    \n",
    "    return summary_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_mnli_tables(mnli_summary, subsetting='genre'):\n",
    "    with open(mnli_summary, 'r') as f:\n",
    "        summary = pd.DataFrame([json.loads(line) for line in f])\n",
    "    \n",
    "    mnli_tables = {}\n",
    "    for comb in summary['comb'].unique():\n",
    "        comb_sum = summary.loc[summary['comb'] == comb, :]\n",
    "\n",
    "        for subset in summary[subsetting].unique():\n",
    "            subset_sum = comb_sum.loc[comb_sum[subsetting] == subset, :]\n",
    "\n",
    "            plot_tab = []\n",
    "            for treat in subset_sum['treat'].unique():\n",
    "                treat_sum = subset_sum.loc[subset_sum['treat'] == treat, :]\n",
    "                s = treat_sum[['iter','acc']].set_index('iter').rename({'acc': treat}, axis=1).transpose()            \n",
    "                plot_tab.append(s)\n",
    "            \n",
    "            mnli_tables[(model, comb, subset)] = pd.concat(plot_tab)\n",
    "    \n",
    "    return summary, mnli_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def split_run_name(run_name, split_by='_'):\n",
    "    name_list = run_name.split(split_by)\n",
    "    if len(name_list) == 2:\n",
    "        input_type = 'full'\n",
    "        comb = 'combined'\n",
    "    elif len(name_list) == 3:\n",
    "        if name_list[-1] == 'hyp':\n",
    "            input_type = name_list[-1]\n",
    "            comb = 'combined'\n",
    "        else:\n",
    "            input_type = 'full'\n",
    "            comb = name_list[-1]\n",
    "    else:\n",
    "        input_type = name_list[-1]\n",
    "        comb = name_list[-2]\n",
    "\n",
    "    return (name_list[0], name_list[1], input_type, comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_sampled_results(sampled_base):\n",
    "    collected = pd.read_csv(os.path.join(sampled_base, 'collected.csv'))\n",
    "    itereval = pd.read_csv(os.path.join(sampled_base, 'itereval.csv'))\n",
    "    mnli = pd.read_csv(os.path.join(sampled_base, 'mnli.csv'))\n",
    "    \n",
    "    return collected, itereval, mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_ttest_pvals(dist_df, verbose=True):\n",
    "    pairs = [\n",
    "        ('baseline', 'LotS'),\n",
    "        ('baseline', 'LitL'),\n",
    "        ('LotS', 'LitL'),\n",
    "    ]\n",
    "    \n",
    "    ttest_dict = {}\n",
    "    for pair in pairs:\n",
    "        a = dist_df.loc[dist_df['treat'] == pair[0], 'acc']\n",
    "        b = dist_df.loc[dist_df['treat'] == pair[1], 'acc']\n",
    "        ttest_dict[pair] = ttest_ind(a, b)\n",
    "    \n",
    "    if verbose:\n",
    "        for pair, ttest_results in ttest_dict.items():\n",
    "            print('='*45)\n",
    "            print(f\"{pair}\\nt: {ttest_results[0]:.5f} | p: {ttest_results[1]/2:.5f}\")\n",
    "    \n",
    "    return ttest_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_round_box(\n",
    "    plot_arrays, \n",
    "    plot_alltraining,\n",
    "    ylim=[0,1],\n",
    "    title=None,\n",
    "    xlabel=None,\n",
    "    ylabel=None,\n",
    "    tabletitle=None,\n",
    "    tableon=True,\n",
    "):\n",
    "    if tableon:\n",
    "        fig, ax = plt.subplots(2, 1)\n",
    "        sns.boxplot(x='treat', y='acc', data=plot_arrays, ax=ax[0])\n",
    "        ax[0].scatter(x=plot_alltraining['treat'], y=plot_alltraining['acc'], c='r', s=75)\n",
    "\n",
    "        ax[0].set_title(title)\n",
    "        ax[0].set_xlabel(xlabel)\n",
    "        ax[0].set_ylabel(ylabel)\n",
    "        ax[0].set_ylim(*ylim)\n",
    "\n",
    "        cell_text = []\n",
    "        order = ['baseline', 'LotS', 'LitL']\n",
    "        for treat in order:\n",
    "            display_text = f'{plot_alltraining.loc[plot_alltraining[\"treat\"] == treat,\"acc\"].values[0]*100:.2f}%'\n",
    "            cell_text.append(display_text)\n",
    "\n",
    "        table = ax[1].table(cellText=[cell_text], colLabels=plot_alltraining['treat'].values, loc='upper center')\n",
    "        table.scale(1, 2)\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        ax[1].set_title(tabletitle)\n",
    "    else:\n",
    "        fig, ax = plt.subplots()\n",
    "        sns.boxplot(x='treat', y='acc', data=plot_arrays, ax=ax)\n",
    "        ax.scatter(x=plot_alltraining['treat'], y=plot_alltraining['acc'], c='r', s=75)\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(xlabel)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_ylim(*ylim)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model='roberta-large-mnli'\n",
    "repo = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "eval_dir = os.path.join(repo, 'eval_summary', model)\n",
    "sample_type = 'cross_eval'\n",
    "iteration = 5\n",
    "\n",
    "mnli_summary = os.path.join(eval_dir, 'mnli_evals', 'eval_summaries.jsonl')\n",
    "\n",
    "plots_dir = os.path.join(eval_dir, 'sample', sample_type, f'r{iteration}', 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_errs, itereval_errs, mnli_errs = load_sampled_results(\n",
    "    os.path.join(eval_dir, 'sample', sample_type, f'r{iteration}')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "collected_errs['treat'] = collected_errs['run'].apply(lambda x: split_run_name(x)[0])\n",
    "collected_errs['iter'] = collected_errs['run'].apply(lambda x: int(split_run_name(x)[1]))\n",
    "collected_errs['mod'] = collected_errs['run'].apply(lambda x: split_run_name(x)[2])\n",
    "collected_errs['combined'] = collected_errs['run'].apply(lambda x: split_run_name(x)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_errs['genre'] = mnli_errs['genre'].fillna('combined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "select2mod = {\n",
    "    ('combined', 'full'): 'combined',\n",
    "    ('combined', 'hyp'): 'hyp',\n",
    "    ('separate', 'full'): 'separate',\n",
    "    ('separate', 'hyp'): 'separate_hyp',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = 'combined'\n",
    "input_type = 'hyp'\n",
    "\n",
    "mod = select2mod[(combined, input_type)]\n",
    "\n",
    "collected = get_val_summary(mod, iteration, eval_dir, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collected_best = collected[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_iter_errs = collected_errs.loc[collected_errs['iter'] == iteration, :]\n",
    "collected_mod_errs = collected_iter_errs.loc[collected_iter_errs['mod'] == input_type, :]\n",
    "collected_err = collected_mod_errs.loc[collected_mod_errs['combined'] == combined, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 20.87870 | p: 0.00000\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 15.65192 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: 0.21880 | p: 0.41451\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9ca68d49d74304b8b94378d8483c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.3,0.7]\n",
    "title=f'{combined} | {input_type} input'\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f'collected-{combined}-{input_type}.{figtype}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = 'separate'\n",
    "input_type = 'hyp'\n",
    "\n",
    "mod = select2mod[(combined, input_type)]\n",
    "\n",
    "collected = get_val_summary(mod, iteration, eval_dir, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "collected_best = collected[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_iter_errs = collected_errs.loc[collected_errs['iter'] == iteration, :]\n",
    "collected_mod_errs = collected_iter_errs.loc[collected_iter_errs['mod'] == input_type, :]\n",
    "collected_err = collected_mod_errs.loc[collected_mod_errs['combined'] == combined, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 25.19341 | p: 0.00000\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 16.37855 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -4.07685 | p: 0.00029\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5d9820ea1c4d06b6b3fb6b88e0b4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.3,0.7]\n",
    "title=f'{combined} | {input_type} input'\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f'collected-{combined}-{input_type}.{figtype}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANS - combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_iterations = 'combined'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexical_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'lexical_overlap',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 2.15641 | p: 0.01856\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 2.46001 | p: 0.00916\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -0.65339 | p: 0.25862\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14de023a1f804f0ca1d1493038bdcd2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'lexical_overlap',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -3.69738 | p: 0.00033\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -5.68781 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -1.41058 | p: 0.08305\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293000c7bb444c61a6d2ec8bfa2b75ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'subsequence',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 2.48796 | p: 0.00856\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 1.82994 | p: 0.03736\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -1.76133 | p: 0.04291\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979efced3b124b2dbe8d0cedb03b8c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'subsequence',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -3.24106 | p: 0.00120\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -7.32403 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -3.13003 | p: 0.00163\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19bcf21ec1d4327b47db144d65a2114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constituent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'constituent',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -4.42137 | p: 0.00004\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 1.97077 | p: 0.02785\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: 5.70621 | p: 0.00000\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea75284db765469cac6f1e833a4b8675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'constituent',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 1.92932 | p: 0.03040\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -0.24893 | p: 0.40235\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -1.94081 | p: 0.02968\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb68554f3e3341e48a3b77671547ea93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HANS - separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_iterations = 'separate'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lexical_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'lexical_overlap',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 2.23237 | p: 0.01563\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 2.58502 | p: 0.00674\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -0.66205 | p: 0.25587\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031e20d089794fdfb9be6fa0fd371243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'lexical_overlap',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -3.95429 | p: 0.00015\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -6.30470 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -1.76212 | p: 0.04285\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3855bfa196134368a2dd6f507f82218f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'subsequence',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 2.56379 | p: 0.00711\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 1.91568 | p: 0.03129\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -1.80483 | p: 0.03932\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17e9b90201246e4a03221f88d6fa106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'subsequence',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -3.51222 | p: 0.00056\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -8.34286 | p: 0.00000\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -3.54752 | p: 0.00050\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d2ed8169874ac4baa089d82ec2f027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constituent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'constituent',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: -3.96532 | p: 0.00015\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 2.29191 | p: 0.01363\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: 5.85281 | p: 0.00000\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf6d93b60f7405ea4cbfba6f64a860d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1.0]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### non-entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_keys = {\n",
    "    'dataset': 'hans',     # either hans or glue\n",
    "    'case': 'constituent',    # combined or specific to respective itereval set\n",
    "    'subcase': 'combined', # combined or specific to respective itereval set\n",
    "    'label': 'non-entailment',   # combined or [entailment, neutral, contradiction] for glue, [entailment, non-entailment] for hans\n",
    "}\n",
    "\n",
    "hans = get_itereval_summary(sub_keys, iteration, eval_dir, combined_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_best = hans[str(iteration)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_errs = itereval_errs.loc[itereval_errs['iter'] == iteration, :]\n",
    "dataset_errs = round_errs.loc[round_errs['dataset'] == sub_keys['dataset'], :]\n",
    "case_errs = dataset_errs.loc[dataset_errs['case'] == sub_keys['case'], :]\n",
    "subcase_errs = case_errs.loc[case_errs['subcase'] == sub_keys['subcase'], :]\n",
    "collected_err = subcase_errs.loc[subcase_errs['label'] == sub_keys['label'], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "for treat in collected_best.index.values:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    accs = accs.append(pd.Series([collected_best[treat]]), ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': collected_best[treat], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 1.44716 | p: 0.07782\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: -0.67058 | p: 0.25317\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -2.09205 | p: 0.02141\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75094c8e4a6401ebdaa94664deb6f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0,1]\n",
    "title=f\"{combined_iterations} | {sub_keys['dataset']} | {sub_keys['case']} | {sub_keys['label']}\"\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle)\n",
    "fig.savefig(os.path.join(plots_dir, f\"{sub_keys['dataset']}-{combined_iterations}-{sub_keys['case']}-{sub_keys['label']}.{figtype}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mnli_summary, 'r') as f:\n",
    "    summary = pd.DataFrame([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_summary = summary.loc[summary['iter'] == str(iteration), :]\n",
    "comb_summary = iter_summary.loc[iter_summary['comb'] == combined, :]\n",
    "genre_summary = comb_summary.loc[comb_summary['tag'] == genre, :] # <--- CHANGE 'tag' to 'genre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_errs = mnli_errs.loc[mnli_errs['iter'] == iteration, :]\n",
    "comb_errs = iter_errs.loc[iter_errs['comb'] == combined, :]\n",
    "collected_err = comb_errs.loc[comb_errs['genre'] == genre, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.899715\n",
      "36     0.894528\n",
      "72     0.888019\n",
      "108    0.892596\n",
      "144    0.898698\n",
      "180    0.878967\n",
      "216    0.899410\n",
      "252    0.886697\n",
      "288    0.901241\n",
      "324    0.897274\n",
      "Name: acc, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(collected_err.loc[collected_err['treat'] == 'LitL', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "order = ['baseline', 'LotS', 'LitL']\n",
    "\n",
    "for treat in order:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    \n",
    "    accs = accs.append(genre_summary.loc[genre_summary['treat'] == treat, 'acc'], ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': genre_summary.loc[genre_summary['treat'] == treat, 'acc'].values[0], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 0.15991 | p: 0.43728\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 0.04122 | p: 0.48376\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -0.11620 | p: 0.45433\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8fe3dfc5cf4701af8fa5074f1d51a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.7,1]\n",
    "title=f'mnli | {combined} | {genre} genre'\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle, tableon=False)\n",
    "fig.savefig(os.path.join(plots_dir, f'mnli-{combined}-{genre}.{figtype}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = 'separate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = 'combined'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mnli_summary, 'r') as f:\n",
    "    summary = pd.DataFrame([json.loads(line) for line in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_summary = summary.loc[summary['iter'] == str(iteration), :]\n",
    "comb_summary = iter_summary.loc[iter_summary['comb'] == combined, :]\n",
    "genre_summary = comb_summary.loc[comb_summary['tag'] == genre, :] # <--- CHANGE 'tag' to 'genre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_errs = mnli_errs.loc[mnli_errs['iter'] == iteration, :]\n",
    "comb_errs = iter_errs.loc[iter_errs['comb'] == combined, :]\n",
    "collected_err = comb_errs.loc[comb_errs['genre'] == genre, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_arrays = []\n",
    "plot_alltraining = []\n",
    "\n",
    "order = ['baseline', 'LotS', 'LitL']\n",
    "\n",
    "for treat in order:\n",
    "    accs = collected_err.loc[collected_err['treat'] == treat, 'acc']\n",
    "    \n",
    "    accs = accs.append(genre_summary.loc[genre_summary['treat'] == treat, 'acc'], ignore_index=True)\n",
    "    accs = accs.to_frame(name='acc')\n",
    "    accs['treat'] = treat\n",
    "    \n",
    "    plot_arrays.append(accs)\n",
    "    plot_alltraining.append({'acc': genre_summary.loc[genre_summary['treat'] == treat, 'acc'].values[0], 'treat': treat})\n",
    "    \n",
    "plot_arrays = pd.concat(plot_arrays, ignore_index=True)\n",
    "plot_alltraining = pd.DataFrame(plot_alltraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "('baseline', 'LotS')\n",
      "t: 0.10853 | p: 0.45733\n",
      "=============================================\n",
      "('baseline', 'LitL')\n",
      "t: 0.06274 | p: 0.47530\n",
      "=============================================\n",
      "('LotS', 'LitL')\n",
      "t: -0.04505 | p: 0.48226\n"
     ]
    }
   ],
   "source": [
    "ttests = get_ttest_pvals(plot_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27122dd1434341eb826adc0a4bd5337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ylim=[0.7,1]\n",
    "title=f'mnli | {combined} | {genre} genre'\n",
    "xlabel=''\n",
    "ylabel='Accuracy'\n",
    "tabletitle='Full Training Data'\n",
    "\n",
    "figtype='jpg'\n",
    "\n",
    "fig = single_round_box(plot_arrays, plot_alltraining, ylim=ylim, title=title, xlabel=xlabel, ylabel=ylabel, tabletitle=tabletitle, tableon=False)\n",
    "fig.savefig(os.path.join(plots_dir, f'mnli-{combined}-{genre}.{figtype}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
